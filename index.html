<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <title>Ggan by jnarayana3</title>
  </head>

  <body>

    <header>
      <div class="container">
        <h1>GAN</h1>
        <h2>Generative Adversarial Network</h2>

        <section id="downloads">
          <a href="https://github.gatech.edu/jnarayana3/ggan" class="btn btn-github"><span class="icon"></span>View on GitHub</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
        
<p>Humans learn by observing and experiencing the physical world. Our brains are very good at prediction and aggregation - if shown 10 different kind of pens, and if presented with a different pen, we can easily identify it as a pen. If asked to imagine a pen, we would imagine one which may not look exactly like any of the ones seen in the past but would still can be classified as a legitimate pen. This task of learning features for a category is one of the prime goals of Machine Learning today. It is not restricted to just images, but can be extended to any form of media like audio, print etc. </p>
<p>Adversarial networks have recently surfaced as a new way to train machines to generate data which is similar to the observed data in the world simply by observing it. This model, popularly known as Generative Adversarial Network (GAN), was proposed and published by Ian Goodfellow in 2016 *0 todo: refernce to paper*. GANs have produced great results in unsupervised and semisupervised learning domain *1 todo: refernce to paper*. Through an innovative combination of computational graphs and game theory they showed that, given enough modeling power, two models fighting against each other would be able to co-train through plain old backpropagation. </p>
<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Architecture</h3>

<p>So how exactly do they work? The two primary components are Generator (G) and Discriminator (D). D is trained to discriminate between real data and fake data. Here real data refers to the real world data in training and fake data is the data produced by G. Both G and D are multilayer perceptron networks. With this framework, both are trained to be better at their job through back propogation and once trained, G is now able to produce data which is very similar to the training data. D is much easier to train than G, more on it below. The input to G can be anything, there have been experiments where a vector of random numbers have been used to produce an image. This truly proves that G has learnt the features of the data. Hence, a GAN consists of 5 main components *todo: add figure with 5 components*:
<ul>
  <li>T, the real data</li>
  <li>G, the generator</li>
  <li>D, the discriminator</li>
  <li>R, the random noise which is the input to G</li>
  <li>A, the training loop which trains the models G and D make each better</li>
</ul>
Before delving into the implimentation and applications, it's essential that one understands the Mathematics behind it. Let the data in T be represented by <MATH>t</MATH>. The aim is to now learn <MATH>p<sub>q</sub></MATH> which is the disctribution of G. But, there is a prior noise which is associated with this, say <MATH>p<sub>z</sub></MATH>, as defined by R. <MATH>G(z; &#952<sub>g</sub>)</MATH> is now the function for the perceptron network for G where <MATH>&#952<sub>g</sub></MATH> is the set of parameters or weights. Let <MATH>D(x; &#952<sub>d</sub>)</MATH> represent the perceptron network for D where <MATH>D(x)</MATH> represents the probability that x is part of the real data <MATH>t</MATH> rather than <MATH>p<sub>g</sub></MATH>. The loss function is now defined to maximize D i.e make D better at telling the difference between real and fake data generated by G. This is done in 2 ways:
<ul>
  <li>maximizing <MATH>D(x)</MATH> over <MATH>p<sub>t</sub></MATH></li>
  <li>miximizing <MATH>D(G(z))</MATH>, i.e minimizing <MATH>1 - D(G(z))</MATH></li>
</ul> 
Using log scale, we can define the reward/value function (in the context of reinforcement learning) as: </br>
<img src="./images/GAN_ValueFunction.png"></br>
As shown in *add reference to 0*, for a fixed <MATH>G</MATH>, the optimum value of <MATH>D(x)</MATH> is given by:</br>
<img class= "two-line" src="./images/opD.png"></br>
Further the author proves that for <MATH>V(G, D)</MATH> to achieve it's global optimum, <MATH>p<sub>g</sub> = p<sub>data</sub> </MATH>. Putting it together, at the global optimum, <MATH>D<sup>*</sup><sub>G</sub> = 1/2</MATH>, which means that the G at this point is able to generate which D is not able to differentiate from orginal data. 
<MATH>D</MATH> is trained using stochastic gradient <b>ascent</b>(since we are <MATH>max D </MATH>) while <MATH>G</MATH> is trained using stochastic gradient <b>descent</b>.
</p>

<h3>
<a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Optimizations</h3>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Applications</h3>

<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h3>
<p> 
  0: https://arxiv.org/pdf/1406.2661v1.pdf </br>
  1: https://arxiv.org/abs/1511.06390
  
</p>
      </section>
    </div>

    
  </body>
</html>
